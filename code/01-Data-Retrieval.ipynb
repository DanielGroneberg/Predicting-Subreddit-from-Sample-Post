{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532ca367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e397264",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': username,\n",
    "    'password': password\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541a036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#create an informative header for your application\n",
    "headers = {'User-Agent': 'namehere/0.0.1'}\n",
    "\n",
    "res = requests.post(\n",
    "    'https://www.reddit.com/api/v1/access_token',\n",
    "    auth=auth,\n",
    "    data=data,\n",
    "    headers=headers)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4b317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve access token\n",
    "token = res.json()['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b789413b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers['Authorization'] = f'bearer {token}'\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a61a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://oauth.reddit.com/r/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fc81e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'../datasets/reddit_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494dbdd5",
   "metadata": {},
   "source": [
    "How to use requests.get() parameters:</br>\n",
    "https://www.w3schools.com/PYTHON/ref_requests_get.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e5982",
   "metadata": {},
   "source": [
    "`old_df` is the existing df to append to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea08131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(old_df):\n",
    "    for s in ['AntiWork', 'Jobs']:\n",
    "        # allow 'after' param to be default first pass\n",
    "        after_index = ''\n",
    "        sub_df = pd.DataFrame()\n",
    "        for f in range(0, 1100, 100):\n",
    "            params = {\n",
    "                    \"limit\": 100,\n",
    "                    'count': f,\n",
    "                    \"after\": after_index\n",
    "            }\n",
    "\n",
    "            res = requests.get(base_url + s, \n",
    "                                   headers = headers, \n",
    "                                   params = params)\n",
    "\n",
    "            res = res.json()\n",
    "            post_count = len(res['data']['children'])\n",
    "            text = [res['data']['children'][i]['data'] for i in range(2, post_count)]\n",
    "            text = [t for t in text if t != '']\n",
    "            # convert text to dataframe format\n",
    "            temp_df = pd.DataFrame(text)\n",
    "            # store temp_df info to subreddit-scale dataframe\n",
    "            sub_df = pd.concat([temp_df, sub_df], axis = 0)\n",
    "            # update after_index\n",
    "            after_index = res['data']['after']\n",
    "            # break loop if the post limit is reached\n",
    "            if after_index is None:\n",
    "                break\n",
    "                \n",
    "        old_df = pd.concat([old_df, sub_df], axis = 0)\n",
    "    df[['subreddit', 'selftext']] = df[['subreddit', 'selftext']].dropna()\n",
    "    old_df = old_df.drop_duplicates(subset = ['selftext'], keep = 'first')\n",
    "    old_df.index = range(len(old_df))\n",
    "    return old_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7aac3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7155, 116)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_posts(df)\n",
    "df.to_csv('../datasets/reddit_dataframe.csv', index_label = False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b4e7cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jobs        0.528236\n",
       "antiwork    0.471764\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the classes are pretty even\n",
    "df['subreddit'].value_counts(normalize = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
